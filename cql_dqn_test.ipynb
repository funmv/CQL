{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dfa63f-3df9-45c6-ac1d-3197421040d5",
   "metadata": {},
   "source": [
    "# CQL-DQN\n",
    "- Q(s,a)를 표현하는 DeepNet 하나를 사용\n",
    "- a의 개수가 정해져 있어야 함: a는 이산공간\n",
    "- (ex.) Cartpole은 2개임 (0 or 1)\n",
    "- (c.f.) a가 연속공간: SAC 사용\n",
    "- (ref) https://github.com/BY571/CQL/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895ad52-73ab-4b79-8987-ad3789be2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import argparse\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from networks import DDQN\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34074938-5104-4f73-b68c-58b36e502e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "babb90f9-c9ef-4b4f-8848-322c9c746d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "eps = 1.\n",
    "d_eps = 1 - 0.01\n",
    "steps = 0\n",
    "average10 = deque(maxlen=10)\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5410f272-e062-489d-94f4-d0cdf93ef53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CQLAgent(state_size=env.observation_space.shape,  # 4\n",
    "                         action_size=env.action_space.n,  # 2(0 or 1 둘 중 하나)\n",
    "                         device=device)\n",
    "\n",
    "buffer = ReplayBuffer(buffer_size=100000, batch_size=32, device=device)\n",
    "collect_random(env=env, dataset=buffer, num_samples=10000)  # 경험 DB: data는 10000개 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ed21e8d-2a1a-4b41-ace7-4d4c3fbce0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(0)]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "episode_steps = 0\n",
    "rewards = 0\n",
    "\n",
    "# 정책에서 추출 또는 랜덤 추출(초기에는 random, 뒤로 갈수록 정책에서)\n",
    "action = agent.get_action(state, epsilon=eps)  \n",
    "print(action)\n",
    "steps += 1\n",
    "next_state, reward, done, truncated, info = env.step(action[0])\n",
    "buffer.add(state, action, reward, next_state, done)\n",
    "loss, cql_loss, bellmann_error = agent.learn(buffer.sample())\n",
    "\n",
    "# state = next_state\n",
    "# rewards += reward\n",
    "# episode_steps += 1\n",
    "# eps = max(1 - ((steps*d_eps)/config.eps_frames), config.min_eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeeb84d-e031-411a-b228-92e40bbf552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# episode_steps = 0\n",
    "# rewards = 0\n",
    "# while True:\n",
    "#     action = agent.get_action(state, epsilon=eps)\n",
    "#     steps += 1\n",
    "#     next_state, reward, done, _ = env.step(action[0])\n",
    "#     buffer.add(state, action, reward, next_state, done)\n",
    "#     loss, cql_loss, bellmann_error = agent.learn(buffer.sample())\n",
    "#     state = next_state\n",
    "#     rewards += reward\n",
    "#     episode_steps += 1\n",
    "#     # min_eps=0.01이고 처음에는 eps가 1, 점차 작아짐\n",
    "#     eps = max(1 - ((steps*d_eps)/config.eps_frames), config.min_eps)  \n",
    "#     if done:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c59ced-925d-49e7-90ff-eed76c4e47c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_config():\n",
    "#     parser = argparse.ArgumentParser(description='RL')\n",
    "#     parser.add_argument(\"--run_name\", type=str, default=\"CQL-DQN\", help=\"Run name, default: CQL-DQN\")\n",
    "#     parser.add_argument(\"--env\", type=str, default=\"CartPole-v0\", help=\"Gym environment name, default: CartPole-v0\")\n",
    "#     parser.add_argument(\"--episodes\", type=int, default=400, help=\"Number of episodes, default: 200\")\n",
    "#     parser.add_argument(\"--buffer_size\", type=int, default=100_000, help=\"Maximal training dataset size, default: 100_000\")\n",
    "#     parser.add_argument(\"--seed\", type=int, default=1, help=\"Seed, default: 1\")\n",
    "#     parser.add_argument(\"--min_eps\", type=float, default=0.01, help=\"Minimal Epsilon, default: 4\")\n",
    "#     parser.add_argument(\"--eps_frames\", type=int, default=1e4, help=\"Number of steps for annealing the epsilon value to the min epsilon, default: 1e5\")\n",
    "#     parser.add_argument(\"--log_video\", type=int, default=0, help=\"Log agent behaviour to wanbd when set to 1, default: 0\")\n",
    "#     parser.add_argument(\"--save_every\", type=int, default=100, help=\"Saves the network every x epochs, default: 25\")\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "94cbbe23-407a-4e96-b289-d334c761eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[3.4076],\n",
       "         [2.9076]]),\n",
       " np.float64(3.40760596444438),\n",
       " np.float64(2.90760596444438))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logsumexp의 연산을 확인하는 부분\n",
    "q_values = torch.tensor([[1.0, 2.0, 3.0], \n",
    "                         [0.5, 2.5, 1.5]])\n",
    "print(q_values.shape)\n",
    "logsumexp = torch.logsumexp(q_values, dim=1, keepdim=True)\n",
    "# logsumexp는 exp(x)값의 연산이 커져서 안정성이 떨어지는 연산을 방지\n",
    "# smoothing된 최대값 계산: 최대값과 비슷하지만 다른 값들도 일부 반영되도록 함\n",
    "logsumexp, np.log(np.exp(1)+np.exp(2)+np.exp(3)), np.log(np.exp(0.5)+np.exp(2.5)+np.exp(1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc274786-cb0e-4b94-9216-9f57cbd27e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQLAgent():\n",
    "    def __init__(self, state_size, action_size, hidden_size=256, device=\"cpu\"):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "        self.tau = 1e-3\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.network = DDQN(state_size=self.state_size,\n",
    "                            action_size=self.action_size,\n",
    "                            layer_size=hidden_size\n",
    "                            ).to(self.device)\n",
    "\n",
    "        self.target_net = DDQN(state_size=self.state_size,\n",
    "                            action_size=self.action_size,\n",
    "                            layer_size=hidden_size\n",
    "                            ).to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.network.parameters(), lr=1e-3)\n",
    "        \n",
    "    \n",
    "    def get_action(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "            self.network.eval()  # Q(s,a): 2개의 a의 값에 따른 Q(보상)의 값\n",
    "            with torch.no_grad():  # Q(s,a): a에 따른 누적보상 합(Q는 s와 a의 함수)\n",
    "                action_values = self.network(state)  # (1) 정책에서 추출하거나\n",
    "            self.network.train()  # Q가 더 큰 a를 선택\n",
    "            action = np.argmax(action_values.cpu().data.numpy(), axis=1)\n",
    "        else:  # (2) 완전하게 random하게 선택하거나\n",
    "            action = random.choices(np.arange(self.action_size), k=1)\n",
    "        return action\n",
    "\n",
    "    def cql_loss(self, q_values, current_action):\n",
    "        \"\"\"Computes the CQL loss for a batch of Q-values and actions.\"\"\"\n",
    "        logsumexp = torch.logsumexp(q_values, dim=1, keepdim=True)  # Q값의 전체적 분포\n",
    "        q_a = q_values.gather(1, current_action)  # 실제 선택한 action의 Q값\n",
    "        \n",
    "        # 두 값의 차이: 선택한 action의 Q값을 보수적으로 학습하도록 강제하는 역할\n",
    "        return (logsumexp - q_a).mean()  \n",
    "\n",
    "    def learn(self, experiences):\n",
    "        # [32,4], [32, 1], [32, 4], [32, 1]\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        with torch.no_grad():  # next_states=[b,4]=[32,4]\n",
    "            Q_targets_next = self.target_net(next_states).detach().max(1)[0].unsqueeze(1)  # [32,2]->[32,1]\n",
    "            # ground truth값을 만듬\n",
    "            Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "             \n",
    "        Q_a_s = self.network(states.to(self.device))  # [32,2]\n",
    "        # 아래는 [32,2]에서 actions(0 or 1)값에 따라 해당하는 위치의 Q값을 가져옴\n",
    "        # Q_a_s=[32,2]인데 actions는 0이나 1이 들어 있는 [32,2]임\n",
    "        #  ==> actions가 0이면 Q_a_s[32,0]이, 1이면 Q_a_s[32,1]이 선정\n",
    "        Q_expected = Q_a_s.gather(1, actions)  \n",
    "        \n",
    "        # 두 Loss값을 정의: CQL loss, MSE loss        \n",
    "        cql1_loss = self.cql_loss(Q_a_s, actions)\n",
    "        bellman_error = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        q1_loss = cql1_loss + 0.5 * bellman_error\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        clip_grad_norm_(self.network.parameters(), 1.)  ################\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.network, self.target_net)\n",
    "        return q1_loss.detach().item(), cql1_loss.detach().item(), bellman_error.detach().item()\n",
    "        \n",
    "        \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d284d397-2323-4ed0-8fc4-57d79356f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(args, save_name, model, wandb, ep=None):\n",
    "    import os\n",
    "    save_dir = './trained_models/' \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    if not ep == None:\n",
    "        torch.save(model.state_dict(), save_dir + args.run_name + save_name + str(ep) + \".pth\")\n",
    "        wandb.save(save_dir + args.run_name + save_name + str(ep) + \".pth\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), save_dir + args.run_name + save_name + \".pth\")\n",
    "        wandb.save(save_dir + args.run_name + save_name + \".pth\")\n",
    "\n",
    "def collect_random(env, dataset, num_samples=200):\n",
    "    state = env.reset()\n",
    "    # pdb.set_trace()\n",
    "    for _ in range(num_samples):\n",
    "        action = env.action_space.sample()\n",
    "        # next_state, reward, done, _ = env.step(action)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        dataset.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e151b82-4950-4085-9d37-97906b83d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.head_1 = nn.Linear(self.input_shape[0], layer_size)\n",
    "        self.ff_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.ff_2 = nn.Linear(layer_size, action_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = torch.relu(self.head_1(input))\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "        out = self.ff_2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0abdaedb-2995-443c-aa51-233beeff551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        # states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        states = torch.from_numpy(np.stack([\n",
    "            e.state[0] if isinstance(e.state, (list, tuple)) and len(e.state) > 1 else e.state\n",
    "            for e in experiences if e is not None\n",
    "        ]))        \n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef114b-b4bf-478b-8c7f-815f025ef42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
